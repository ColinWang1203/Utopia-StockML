from Utopia_tools import *

import matplotlib.pyplot as plt
import numpy as np
import random
from sklearn import datasets, linear_model
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR


# P_enable_logging()

All_data = [
[130, 225, 13, 368, 3.0, 88, 8.5]
,[130, 225, 13, 368, 32.1, 15, 3.8]
,[130, 225, 13, 368, -0.6, -17, 3.0]
,[130, 225, 13, 368, -17.0, 29, 2.8]
,[130, 225, 13, 368, 16.0, 72, 2.6]
,[130, 225, 13, 368, 19.5, 34, 2.5]
,[130, 225, 13, 368, 3.1, 4, 2.5]
,[130, 225, 13, 368, -8.0, 16, 2.3]
,[130, 225, 13, 368, 17.9, 55, 2.2]
,[130, 225, 13, 368, 8.1, 125, 2.1]
,[130, 225, 13, 368, 18.5, 61, 2.0]
,[130, 225, 13, 368, -18.5, 8, 1.9]
,[130, 225, 13, 368, 38.9, 34, 1.8]
,[130, 225, 13, 368, -34.5, 18, 1.8]
,[130, 225, 13, 368, -1.8, 31, 1.7]
,[130, 225, 13, 368, 14.1, 34, 1.7]
,[130, 225, 13, 368, -14.5, 9, 1.6]
,[130, 225, 13, 368, -14.8, 84, 1.6]
,[130, 225, 13, 368, 12.8, 69, 1.6]
,[130, 225, 13, 368, 83.4, 117, 1.6]
,[130, 225, 13, 368, -10.7, 36, 1.6]
,[130, 225, 13, 368, 2.5, 11, 1.5]
,[130, 225, 13, 368, 1.7, 24, 1.5]
,[130, 225, 13, 368, -2.9, 37, 1.4]
,[130, 225, 13, 368, 8.9, 73, 1.4]
,[130, 225, 13, 368, -8.4, 5, 1.4]
,[130, 225, 13, 368, 37.3, 17, 1.4]
,[130, 225, 13, 368, 10.3, 9, 1.4]
,[130, 225, 13, 368, 8.4, 13, 1.4]
,[130, 225, 13, 368, 33.8, 21, 1.3]
,[130, 225, 13, 368, 8.8, 20, 1.3]
,[130, 225, 13, 368, 1.7, 29, 1.3]
,[130, 225, 13, 368, 21.8, 16, 1.3]
,[130, 225, 13, 368, 4.4, 15, 1.2]
,[130, 225, 13, 368, -10.7, 37, 1.2]
,[130, 225, 13, 368, 22.4, 1, 1.2]
,[130, 225, 13, 368, 32.5, 42, 1.2]
,[130, 225, 13, 368, -6.8, 35, 1.2]
,[130, 225, 13, 368, -26.4, 7, 1.1]
,[130, 225, 13, 368, -23.5, 15, 1.1]
,[130, 225, 13, 368, 9.1, 3, 1.1]
,[130, 225, 13, 368, 8.4, 22, 1.1]
,[130, 225, 13, 368, 15.4, 12, 1.1]
,[130, 225, 13, 368, -8.9, 7, 1.1]
,[130, 225, 13, 368, -5.2, 2, 1.1]
,[130, 225, 13, 368, 4.0, -2, 1.1]
,[130, 225, 13, 368, -33.4, 0, 1.1]
,[130, 225, 13, 368, -6.6, -2, 1.1]
,[130, 225, 13, 368, 19.1, -4, 1.1]
,[130, 225, 13, 368, 12.2, 17, 1.1]
,[130, 225, 13, 368, 1.5, -9, 1.1]
,[130, 225, 13, 368, -15.4, 3, 1.0]
,[130, 225, 13, 368, -36.7, 10, 1.0]
,[130, 225, 13, 368, -35.2, -2, 1.0]
,[130, 225, 13, 368, -3.1, 15, 1.0]
,[130, 225, 13, 368, -17.9, 11, 1.0]
,[130, 225, 13, 368, 1.8, -56, 1.0]
,[130, 225, 13, 368, 17.9, 6, 1.0]
,[130, 225, 13, 368, 8.8, 3, 1.0]
,[130, 225, 13, 368, -6.4, 0, 1.0]
,[130, 225, 13, 368, 16.3, 2, 1.0]
,[130, 225, 13, 368, 5.4, 0, 1.0]
,[130, 225, 13, 368, 58.4, 4, 1.0]
,[130, 225, 13, 368, 24.7, -4, 1.0]
,[130, 225, 13, 368, 19.5, -6, 1.0]
,[130, 225, 13, 368, 12.1, -12, 1.0]
,[130, 225, 13, 368, 9.8, 11, 1.0]
,[130, 225, 13, 368, 13.2, -7, 1.0]
,[130, 225, 13, 368, 9.7, 9, 1.0]
,[130, 225, 13, 368, -7.9, 0, 0.9]
,[130, 225, 13, 368, -19.2, 5, 0.9]
,[130, 225, 13, 368, -20.3, -10, 0.9]
,[130, 225, 13, 368, -15.6, 7, 0.9]
,[130, 225, 13, 368, 9.5, -8, 0.9]
,[130, 225, 13, 368, -15.7, 9, 0.9]
,[130, 225, 13, 368, -18.4, 11, 0.9]
,[130, 225, 13, 368, 8.2, 5, 0.9]
,[130, 225, 13, 368, 20.4, 2, 0.9]
,[130, 225, 13, 368, 3.3, 0, 0.9]
,[130, 225, 13, 368, -16.8, 12, 0.9]
,[130, 225, 13, 368, 0.9, 6, 0.9]
,[130, 225, 13, 368, 3.1, -3, 0.9]
,[130, 225, 13, 368, 11.5, -3, 0.9]
,[130, 225, 13, 368, 3.9, -2, 0.9]
,[130, 225, 13, 368, 24.9, 7, 0.9]
,[130, 225, 13, 368, 22.9, -8, 0.9]
,[130, 225, 13, 368, -34.4, 1, 0.9]
,[130, 225, 13, 368, -11.7, 12, 0.9]
,[130, 225, 13, 368, -18.7, 4, 0.8]
,[130, 225, 13, 368, -33.8, 0, 0.8]
,[130, 225, 13, 368, 3.0, -8, 0.8]
,[130, 225, 13, 368, -20.0, -3, 0.8]
,[130, 225, 13, 368, 0.3, -12, 0.8]
,[130, 225, 13, 368, -14.7, 1, 0.8]
,[130, 225, 13, 368, -13.4, -1, 0.8]
,[130, 225, 13, 368, -15.8, -6, 0.8]
,[130, 225, 13, 368, -8.2, -16, 0.8]
,[130, 225, 13, 368, 28.4, 4, 0.8]
,[130, 225, 13, 368, -1.9, -17, 0.8]
,[130, 225, 13, 368, 41.2, 1, 0.8]
,[130, 225, 13, 368, -5.2, 15, 0.8]
,[130, 225, 13, 368, -0.6, 9, 0.8]
,[130, 225, 13, 368, 4.6, 2, 0.8]
,[130, 225, 13, 368, 4.4, -9, 0.8]
,[130, 225, 13, 368, -4.4, -17, 0.7]
,[130, 225, 13, 368, -26.8, -7, 0.7]
,[130, 225, 13, 368, -8.1, 12, 0.7]
,[130, 225, 13, 368, -21.8, 7, 0.7]
,[130, 225, 13, 368, 22.9, -5, 0.7]
,[130, 225, 13, 368, 29.6, -24, 0.7]
,[130, 225, 13, 368, -6.5, 8, 0.7]
,[130, 225, 13, 368, -16.2, -4, 0.7]
,[130, 225, 13, 368, 17.9, 5, 0.7]
,[130, 225, 13, 368, -0.4, -16, 0.7]
,[130, 225, 13, 368, -18.0, 14, 0.6]
,[130, 225, 13, 368, 2.6, -12, 0.6]
,[130, 225, 13, 368, 28.3, -2, 0.6]
,[130, 225, 13, 368, 100, -33, 0.6]
,[130, 225, 13, 368, -23.3, 3, 0.6]
,[130, 225, 13, 368, 4.5, -15, 0.6]
,[130, 225, 13, 368, -7.2, -4, 0.6]
,[130, 225, 13, 368, 100, -13, 0.6]
,[130, 225, 13, 368, 13.7, -18, 0.5]
,[130, 225, 13, 368, 1.0, -3, 0.5]
,[130, 225, 13, 368, -15.4, 0, 0.5]
,[130, 225, 13, 368, 17.3, -9, 0.4]
,[130, 225, 13, 368, -9.1, -21, 0.3]
,[130, 225, 13, 368, 100, -44, 0.3]
,[130, 225, 13, 368, 4.7, -13, 0.3]
,[130, 225, 13, 368, -6.5, -9, 0.3]
]

# remove this afterward
# All_data = [[a for a in b[0:7]] for b in All_data]


with open('model_random_forest.pickle', 'rb') as f:
    model = pickle.load(f)
final_input_today_list = All_data
today_input = final_input_today_list
print('today input:')
print(today_input)
final = []
greater_zero_c = 0
for ti in today_input:
    print(ti)
    predict_result = model.predict([ti])
    final.append([predict_result,ti])
    if predict_result > 0:
        greater_zero_c += 1
print('------')
final.sort(key=lambda x: x[0])
P_printl(final)
print('greater_zero rate = '+str(greater_zero_c/len(today_input)))




P()






# All_data = [[a for a in b[0:8]]+[a for a in b[8:]] for b in All_data]

# print(All_data)

# CHECK MODEL IF NORMAL


# t = [[0.1,0.2],[0.3,0.4],[0.3,0.6],[0.2,0.5],[0.06,0.07],[0.12,0.05]]
# m = [0.3,0.7,0.9,0.7,0.14,0.17]
# # scalar1 = StandardScaler()
# # scalar1.fit(t)
# # t = scalar1.transform(t)
# # scalar2 = StandardScaler()
# # scalar2.fit(m)
# # m = scalar2.transform(m)
# # print(t)
# # print(m)


# # model = RandomForestRegressor(n_estimators=1000, oob_score=True, random_state=1000)
# # model = LogisticRegression(C=1000.0, random_state=0)
# # model = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,
# #                coef0=1)
# model = MLPRegressor(hidden_layer_sizes=(10), activation="relu",
#                  solver='lbfgs', alpha=0.001,
#                  batch_size='auto', learning_rate="constant",
#                  learning_rate_init=0.01,
#                  power_t=0.5, max_iter=2000,tol=1e-4)
# model.fit(t, m)
# # tt = [[2,3],[4,5]]
# # mm = [5,9]
# # tt = scalar1.transform([[0.3,0.4]])
# # mm = model.predict(tt)
# # mm = scalar2.inverse_transform(mm)
# # print(tt)
# # print(mm)
# # print(model.score(tt,mm))
# tt = [[0.2,0.3],[0.4,0.5]]
# mm = [0.5,0.9]
# print(model.score(tt,mm))
# print(model.predict([[0.3,0.55]]))

# MAIN

np.random.shuffle(All_data)

x = [ a[0:7] for a in All_data[:(len(All_data)-20)]]
y = [ a[7] for a in All_data[:(len(All_data)-20)]]

x, y = np.array(x), np.array(y)
print('Median is '+str(np.median(y)))
model_ori = RandomForestRegressor()

model_ori.fit(x, y)
with open('model_random_forest.pickle', 'wb') as f:
    pickle.dump(model_ori, f)
with open('model_random_forest.pickle', 'rb') as f:
    model = pickle.load(f)
r_sq = model.score(x, y)
result = []
result_guess = []
guess_data = [ a[7] for a in All_data[-20:]]
# print(guess_data)
# print(np.median(guess_data))
# print(np.mean(guess_data))
guess_low_range = np.median(guess_data)-np.mean(guess_data)/2
guess_high_range = np.median(guess_data)+np.mean(guess_data)/2
print(guess_low_range)
print(guess_high_range)

for i in range(1,20):
    # print('-----------------')
    x =  [All_data[-i][0:7]]
    y = All_data[-i][7]
    y_pred = model.predict(x)
    y_guess = random.uniform(guess_low_range,guess_high_range)
    result.append(abs(y_pred - y))
    result_guess.append(abs(y_guess - y))
    # print(y)
    # print(y_pred)
    # print(y_guess)

a=[ a[0:7] for a in All_data[-20:]]
b=[ a[7] for a in All_data[-20:]]
for aa,bb in zip(a,b):
    # print('input:')
    # print(aa)
    output = model.predict([aa])
    if output > 3:
        print('-----------------')
        print('answer:')
        print(bb)
        print('output:')
        print(output)
print('-----------------')
print('Guess error is '+str(sum(result_guess)/len(result_guess)))
print('Prediction error is '+str(sum(result)/len(result)))
print('Score is '+str(model.score(a,b)))

# a=[ a[0:7] for a in All_data[-3:]]
# b=[ a[7] for a in All_data[-3:]]
# print(a)
# print(b)
# today_input = a
# final = []
# for ti in today_input:
#     print(ti)
#     final.append([model.predict([ti]),ti])
# final.sort(key=lambda x: x[0], reverse=True)
# print(final)

today_input = [
[109, 285, 37, 431, -18.5, 1, 13.1]
,[109, 285, 37, 431, -29.8, 1, 6.3]
,[109, 285, 37, 431, 1.7, 1, 4.4]
,[109, 285, 37, 431, -10.7, 1, 4.1]
,[109, 285, 37, 431, 14.4, 1, 3.8]
,[109, 285, 37, 431, 36.6, 1, 3.7]
,[109, 285, 37, 431, -5.7, 1, 3.2]
,[109, 285, 37, 431, -5.6, 1, 3.0]
,[109, 285, 37, 431, 4.6, 1, 2.9]
,[109, 285, 37, 431, 41.2, 1, 2.8]
,[109, 285, 37, 431, 21.0, 1, 2.4]
,[109, 285, 37, 431, -4.4, 1, 2.4]
,[109, 285, 37, 431, -0.6, 1, 2.4]
,[109, 285, 37, 431, 8.8, 1, 2.2]
,[109, 285, 37, 431, 24.4, 1, 2.1]
,[109, 285, 37, 431, -6.6, 1, 2.1]
,[109, 285, 37, 431, 12.2, 1, 2.1]
,[109, 285, 37, 431, 12.6, 1, 2.0]
,[109, 285, 37, 431, 8.8, 1, 2.0]
,[109, 285, 37, 431, 3.3, 1, 1.9]
,[109, 285, 37, 431, -9.3, 1, 1.9]
,[109, 285, 37, 431, -8.4, 1, 1.9]
,[109, 285, 37, 431, -8.2, 1, 1.9]
,[109, 285, 37, 431, 5.7, 1, 1.9]
,[109, 285, 37, 431, 24.9, 1, 1.9]
,[109, 285, 37, 431, -13.5, 1, 1.8]
,[109, 285, 37, 431, -26.8, 1, 1.8]
,[109, 285, 37, 431, 24.9, 1, 1.8]
,[109, 285, 37, 431, -14.9, 1, 1.8]
,[109, 285, 37, 431, -16.0, 1, 1.8]
,[109, 285, 37, 431, 19.5, 1, 1.8]
,[109, 285, 37, 431, -33.4, 1, 1.8]
,[109, 285, 37, 431, -6.8, 1, 1.8]
,[109, 285, 37, 431, 1.0, 1, 1.8]
,[109, 285, 37, 431, 13.2, 1, 1.8]
,[109, 285, 37, 431, -20.0, 1, 1.7]
,[109, 285, 37, 431, -13.3, 1, 1.7]
,[109, 285, 37, 431, -3.8, 1, 1.7]
,[109, 285, 37, 431, 19.1, 1, 1.7]
,[109, 285, 37, 431, -36.7, 1, 1.6]
,[109, 285, 37, 431, -12.7, 1, 1.6]
,[109, 285, 37, 431, -10.6, 1, 1.6]
,[109, 285, 37, 431, -0.2, 1, 1.6]
,[109, 285, 37, 431, -42.4, 1, 1.6]
,[109, 285, 37, 431, 19.1, 1, 1.6]
,[109, 285, 37, 431, 3.1, 1, 1.6]
,[109, 285, 37, 431, 17.9, 1, 1.6]
,[109, 285, 37, 431, -10.3, 1, 1.6]
,[109, 285, 37, 431, 16.2, 1, 1.6]
,[109, 285, 37, 431, 19.1, 1, 1.6]
,[109, 285, 37, 431, -19.9, 1, 1.5]
,[109, 285, 37, 431, 0.3, 1, 1.5]
,[109, 285, 37, 431, -16.1, 1, 1.5]
,[109, 285, 37, 431, -1.9, 1, 1.5]
,[109, 285, 37, 431, 58.4, 1, 1.5]
,[109, 285, 37, 431, 16.0, 1, 1.5]
,[109, 285, 37, 431, 6.1, 1, 1.5]
,[109, 285, 37, 431, -5.8, 1, 1.4]
,[109, 285, 37, 431, -5.7, 1, 1.4]
,[109, 285, 37, 431, 3.7, 1, 1.4]
,[109, 285, 37, 431, -17.7, 1, 1.4]
,[109, 285, 37, 431, -18.0, 1, 1.4]
,[109, 285, 37, 431, 6.5, 1, 1.4]
,[109, 285, 37, 431, 8.4, 1, 1.4]
,[109, 285, 37, 431, 3.4, 1, 1.3]
,[109, 285, 37, 431, -4.5, 1, 1.3]
,[109, 285, 37, 431, 7.2, 1, 1.3]
,[109, 285, 37, 431, -16.2, 1, 1.3]
,[109, 285, 37, 431, -50.4, 1, 1.3]
,[109, 285, 37, 431, -20.1, 1, 1.3]
,[109, 285, 37, 431, 27.4, 1, 1.3]
,[109, 285, 37, 431, 24.7, 1, 1.3]
,[109, 285, 37, 431, -1.8, 1, 1.3]
,[109, 285, 37, 431, 10.9, 1, 1.3]
,[109, 285, 37, 431, 12.8, 1, 1.2]
,[109, 285, 37, 431, 1.4, 1, 1.2]
,[109, 285, 37, 431, -8.5, 1, 1.2]
,[109, 285, 37, 431, -2.7, 1, 1.2]
,[109, 285, 37, 431, -9.0, 1, 1.2]
,[109, 285, 37, 431, 3.7, 1, 1.2]
,[109, 285, 37, 431, 37.7, 1, 1.2]
,[109, 285, 37, 431, -14.8, 1, 1.2]
,[109, 285, 37, 431, -20.9, 1, 1.2]
,[109, 285, 37, 431, 0.3, 1, 1.2]
,[109, 285, 37, 431, -10.5, 1, 1.1]
,[109, 285, 37, 431, 2.2, 1, 1.1]
,[109, 285, 37, 431, -5.2, 1, 1.1]
,[109, 285, 37, 431, -28.9, 1, 1.1]
,[109, 285, 37, 431, 1.3, 1, 1.0]
,[109, 285, 37, 431, 16.6, 1, 1.0]
,[109, 285, 37, 431, 17.2, 1, 1.0]
,[109, 285, 37, 431, -15.6, 1, 1.0]
,[109, 285, 37, 431, 15.5, 1, 1.0]
,[109, 285, 37, 431, 31.4, 1, 0.9]
,[109, 285, 37, 431, -6.9, 1, 0.9]
,[109, 285, 37, 431, -24.5, 1, 0.9]
,[109, 285, 37, 431, 1.8, 1, 0.9]
,[109, 285, 37, 431, 3.4, 1, 0.9]
,[109, 285, 37, 431, 17.3, 1, 0.8]
,[109, 285, 37, 431, 55.7, 1, 0.8]
,[109, 285, 37, 431, 6.6, 1, 0.8]
,[109, 285, 37, 431, 3.1, 1, 0.7]
,[109, 285, 37, 431, 10.1, 1, 0.7]
,[109, 285, 37, 431, -21.7, 1, 0.6]
,[109, 285, 37, 431, -16.4, 1, 0.6]
,[109, 285, 37, 431, 55.7, 1, 0.6]
,[109, 285, 37, 431, 53.9, 1, 0.6]
,[109, 285, 37, 431, 9.8, 1, 0.6]
,[109, 285, 37, 431, 12.1, 1, 0.5]
]
final = []
for ti in today_input:
    print(ti)
    final.append([model.predict([ti]),ti])
final.sort(key=lambda x: x[0])
P_printl(final)









#  STRESS TEST

# TURE_FINAL = []
# for mm in range(1,11):
#     n_estimators_c = mm * 100
#     for nn in range(1,11):
#         random_state_c = nn * 100
#         final_result_list = []
#         for n in range(1,10):

#             np.random.shuffle(All_data)

#             x = [ a[0:7] for a in All_data[:(len(All_data)-20)]]
#             y = [ a[7] for a in All_data[:(len(All_data)-20)]]

#             x, y = np.array(x), np.array(y)
#             # P_printl(x)
#             # print(y)

#             # model = linear_model.LinearRegression().fit(x, y)
#             model = RandomForestRegressor(n_estimators=n_estimators_c, oob_score=True, random_state=random_state_c, warm_start = True)
#             model.fit(x, y)
#             r_sq = model.score(x, y)
#             # print('coefficient of determination:', r_sq)
#             # print('intercept:', model.intercept_)
#             # print('slope:', model.coef_)
#             result = []
#             result_guess = []
#             for i in range(1,20):
#                 x =  [All_data[-i][0:7]]
#                 y = All_data[-i][7]
#                 y_pred = model.predict(x)
#                 result.append(abs(y_pred - y))

#             final_result_list.append(sum(result)/len(result))


#         TURE_FINAL.append([n_estimators_c,random_state_c,(sum(final_result_list)/len(final_result_list))])
# print(TURE_FINAL)